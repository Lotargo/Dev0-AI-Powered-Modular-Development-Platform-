"""
This module implements the new LLM-powered Verifier agent.

The Verifier's role is to act as a "smart" quality gate for the MVPs
generated by the Architect. It follows a two-pronged strategy:
1.  **Runtime Analysis:** It first executes the MVP to catch any runtime
    errors (e.g., syntax errors, import errors, crashes).
2.  **Functional Testing (QA):** If the MVP runs successfully, it prompts an LLM
    to generate a functional test script based on the user's task and executes it
    against the running application.
"""

import os
import shutil
import subprocess
import time
import pydantic
import re
from typing import Optional
from project.core.llm_gateway.gateway import execute as llm_execute
from project.modules.filesystem.read_file import execute as read_file, ReadFileInput
from project.modules.filesystem.create_file import execute as create_file, CreateFileInput

class Input(pydantic.BaseModel):
    project_path: str
    task_prompt: str # The original task prompt is now needed for logical analysis
    model_group: Optional[str] = pydantic.Field("reasoning_model_group", description="The model group to use.")

class Output(pydantic.BaseModel):
    status: str # "success" or "error"
    feedback: str # Detailed feedback for the Architect

async def execute_async(input_data: Input) -> Output:
    """
    Executes the LLM-powered verification process.
    """
    project_path = input_data.project_path
    group_to_use = input_data.model_group if input_data.model_group else "reasoning_model_group"

    server_process = None

    try:
        # --- 1. Runtime Analysis (Start Server) ---
        print(f"--- Verifier: Starting runtime analysis of project at '{project_path}' ---")

        # Install dependencies
        try:
            subprocess.run(
                ['poetry', 'install'],
                cwd=project_path,
                capture_output=True,
                text=True,
                check=True
            )
        except subprocess.CalledProcessError as e:
             print(f"--- Verifier: Dependency installation failed! ---\n{e.stderr}")
             raise e

        # Start Server in Background
        print("--- Verifier: Starting MVP server... ---")
        server_process = subprocess.Popen(
            ['poetry', 'run', 'python', 'run.py'],
            cwd=project_path,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )

        # Wait a moment for startup
        time.sleep(5)

        # Check if it died
        if server_process.poll() is not None:
            stdout, stderr = server_process.communicate()
            # Include stdout in the error message as some scripts print errors there
            raise subprocess.CalledProcessError(server_process.returncode, "run.py", output=stdout, stderr=stderr)

        print("--- Verifier: Server started successfully. Proceeding to Functional QA. ---")

        # --- 2. Functional QA (Generate & Run Tests) ---

        prompt = f"""
        You are a QA Automation Engineer.
        Your task is to write a standalone Python script to verify a running web application.

        **User Task (Requirements):** "{input_data.task_prompt}"

        **Environment:**
        - The application is running locally at http://localhost:8000.
        - It is likely a FastAPI app.

        **Instructions:**
        1. Write a Python script using `httpx` (or `requests`) to test the endpoints implied by the User Task.
        2. Use standard `assert` statements to verify responses (status codes, JSON content).
        3. If the task mentions specific logic (e.g. "URL shortener"), test that flow (create -> verify result).
        4. The script should exit with code 0 on success, and non-zero on assertion failure.
        5. DO NOT assume any specific implementation details (like variable names), test only the public API behavior.
        6. Include `print("SUCCESS")` at the end.

        **Output:**
        Return ONLY the raw Python code for the test script. No markdown, no explanations.
        """

        qa_code = await llm_execute(prompt=prompt, model_group=group_to_use)

        # Strip markdown if present
        qa_code = re.sub(r'^```python\n', '', qa_code, flags=re.MULTILINE)
        qa_code = re.sub(r'^```\n', '', qa_code, flags=re.MULTILINE)
        qa_code = re.sub(r'\n```$', '', qa_code, flags=re.MULTILINE)

        test_script_path = os.path.join(project_path, "qa_verify.py")
        with open(test_script_path, "w") as f:
            f.write(qa_code)

        print(f"--- Verifier: QA script generated at {test_script_path}. Running... ---")

        # Ensure httpx is installed
        subprocess.run(
            ['poetry', 'run', 'pip', 'install', 'httpx'],
            cwd=project_path,
            capture_output=True,
            check=False
        )

        qa_process = subprocess.run(
            ['poetry', 'run', 'python', 'qa_verify.py'],
            cwd=project_path,
            capture_output=True,
            text=True
        )

        if qa_process.returncode == 0:
            print("--- Verifier: QA Tests PASSED. ---")
            return Output(status="success", feedback="QA Functional Tests Passed.")
        else:
            print("--- Verifier: QA Tests FAILED. ---")
            feedback = f"Functional Test Failed:\nSTDOUT:\n{qa_process.stdout}\nSTDERR:\n{qa_process.stderr}"
            print(feedback)

            # Cleanup
            if server_process:
                server_process.terminate()
            shutil.rmtree(project_path)

            return Output(status="error", feedback=feedback)

    except subprocess.CalledProcessError as e:
        # --- Handle Runtime Error ---
        print("--- Verifier: Runtime analysis failed. Generating feedback. ---")

        # Capture both stdout and stderr
        error_output = f"STDERR:\n{e.stderr}\nSTDOUT:\n{e.output}"
        print(f"--- Runtime Output ---\n{error_output}\n-------------------------")

        prompt = f"""
        As a senior software engineer, your task is to analyze a runtime error from a generated code solution.

        **Original Task:**
        {input_data.task_prompt}

        **Runtime Error Logs:**
        ```
        {error_output}
        ```

        **Analysis:**
        Based on the error, provide a clear, concise, and actionable explanation of what went wrong and what the original developer needs to fix in their code.
        Focus on the root cause (e.g. missing imports, wrong paths, syntax errors).
        """

        llm_feedback = await llm_execute(
            prompt=prompt,
            model_group=group_to_use
        )

        # Clean up the failed project
        # shutil.rmtree(project_path) # DEBUG: Keep failed projects for inspection
        print(f"--- Verifier: Runtime error found. Deleting project at '{project_path}' ---")

        return Output(status="error", feedback=llm_feedback)

    except Exception as e:
        # --- Handle unexpected errors ---
        print(f"--- An unexpected error occurred in the Verifier: {e} ---")
        if server_process:
            server_process.terminate()
        if os.path.exists(project_path):
            shutil.rmtree(project_path)
        return Output(status="error", feedback=f"An unexpected error occurred in the Verifier itself: {str(e)}")
    finally:
        if server_process and server_process.poll() is None:
            server_process.terminate()
