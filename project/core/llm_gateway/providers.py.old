import httpx
import uuid
import time
import json as json_lib
from typing import Dict, Any

from .models import ChatCompletionRequest, ChatCompletionResponse, ChatCompletionMessage, ChatCompletionChoice, ToolCall, FunctionCall

async def proxy_google_chat(req: ChatCompletionRequest, model_name: str, key: str) -> ChatCompletionResponse:
    api_url = f"https://generativelanguage.googleapis.com/v1beta/models/{model_name}:generateContent?key={key}"

    google_contents = []
    for msg in req.messages:
        role = "model" if msg["role"] == "assistant" else "user"
        google_contents.append({"role": role, "parts": [{"text": msg["content"]}]})

    payload = {"contents": google_contents}

    if req.tools:
        # Gemini uses a slightly different format for tool definitions
        payload["tools"] = [{"function_declarations": [tool.function.model_dump() for tool in req.tools]}]
        # Gemini's tool_choice equivalent is tool_config
        if req.tool_choice and req.tool_choice != "auto":
             payload["tool_config"] = {"function_calling_config": {"mode": "ANY"}}


    async with httpx.AsyncClient() as client:
        response = await client.post(api_url, json=payload, timeout=300.0)
    response.raise_for_status()
    google_resp = response.json()

    candidate = google_resp.get("candidates", [{}])[0]
    content = candidate.get("content", {})
    parts = content.get("parts", [{}])

    tool_calls = []
    message_content = None

    for part in parts:
        if "functionCall" in part:
            fc = part["functionCall"]
            tool_calls.append(ToolCall(
                id=f"call_{uuid.uuid4().hex}", # Gemini doesn't provide a tool call ID
                function=FunctionCall(
                    name=fc["name"],
                    arguments=json_lib.dumps(fc.get("args", {}))
                )
            ))
        if "text" in part:
            message_content = part["text"]

    final_message = ChatCompletionMessage(role="assistant", content=message_content, tool_calls=tool_calls if tool_calls else None)
    final_choice = ChatCompletionChoice(index=0, message=final_message, finish_reason="stop")
    usage = google_resp.get("usageMetadata", {"promptTokenCount": 0, "candidatesTokenCount": 0, "totalTokenCount": 0})

    return ChatCompletionResponse(
        id=f"chatcmpl-{uuid.uuid4().hex}",
        created=int(time.time()),
        model=req.model,
        choices=[final_choice],
        usage={
            "prompt_tokens": usage.get("promptTokenCount", 0),
            "completion_tokens": usage.get("candidatesTokenCount", 0),
            "total_tokens": usage.get("totalTokenCount", 0)
        }
    )

async def proxy_mistral_chat(req: ChatCompletionRequest, model_name: str, key: str) -> ChatCompletionResponse:
    api_url = "https://api.mistral.ai/v1/chat/completions"
    headers = {"Authorization": f"Bearer {key}"}

    payload = req.model_dump(exclude={'stream'}, exclude_none=True)
    payload['model'] = model_name

    # Mistral uses the OpenAI-compatible `tools` format
    if req.tools:
        payload["tools"] = [tool.model_dump() for tool in req.tools]
        payload["tool_choice"] = req.tool_choice

    async with httpx.AsyncClient() as client:
        response = await client.post(api_url, json=payload, headers=headers, timeout=300.0)

    response.raise_for_status()
    response_data = response.json()

    # Manually parse to our pydantic models to handle tool_calls
    choices = []
    for choice_data in response_data.get("choices", []):
        message_data = choice_data.get("message", {})
        message = ChatCompletionMessage(**message_data)
        choices.append(ChatCompletionChoice(index=choice_data.get("index"), message=message))

    return ChatCompletionResponse(
        id=response_data.get("id"),
        created=response_data.get("created"),
        model=response_data.get("model"),
        choices=choices,
        usage=response_data.get("usage")
    )
